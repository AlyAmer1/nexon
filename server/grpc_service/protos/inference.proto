// File: server/grpc_service/protos/inference.proto
// Version: 1.1 | Author: Aly Amer | Date: 04.09.2025
// Defines the gRPC service contract for the NEXON AI Inference Platform.
syntax = "proto3";

package nexon.grpc.inference;

// The InferenceService provides a Unary RPC for performing inference on a deployed model.
service InferenceService {
  // Predict performs inference on a given model version.
  rpc Predict(PredictRequest) returns (PredictReply);
}

// Enumerates the tensor element data types supported by the service.
enum DataType {
  DT_UNSPECIFIED = 0;
  DT_FLOAT32 = 1;
  DT_FLOAT64 = 2;
  DT_INT32 = 3;
  DT_INT64 = 4;
  DT_BOOL = 5;
  DT_STRING = 6;
}

// Request message for the Predict RPC.
message PredictRequest {
  // The unique name of the model to be used for inference.
  string model_name = 1;

  // The specific version of the model to use.
  int32 model_version = 2;

  // The list of input tensors for the model.
  repeated Tensor inputs = 3;
}

// Reply message for the Predict RPC.
message PredictReply {
  // The list of output tensors produced by the model.
  repeated Tensor outputs = 1;
}

// Describes an N-dimensional tensor, mirroring the structure needed by ONNX Runtime.
message Tensor {
  // The shape of the tensor (e.g., [1, 3, 224, 224]).
  repeated int64 dims = 1;

  // The data type of the tensor elements.
  DataType data_type = 2;

  // The optional name of the tensor, used by some models to map inputs/outputs.
  string name = 3;

  // The raw tensor content is serialized into a single bytes field.
  // This is the most efficient way to transmit large numerical arrays.
  // The 'dims' and 'data_type' fields provide the metadata to reconstruct
  // the original tensor on the server side.
  bytes tensor_content = 4;
}

